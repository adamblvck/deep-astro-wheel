{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neural-Network\" data-toc-modified-id=\"Neural-Network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Neural Network</a></span></li><li><span><a href=\"#Ok-cool\" data-toc-modified-id=\"Ok-cool-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Ok cool</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37289116.0\n",
      "1 37908056.0\n",
      "2 43386976.0\n",
      "3 44586468.0\n",
      "4 35570444.0\n",
      "5 20559140.0\n",
      "6 9378759.0\n",
      "7 4132521.5\n",
      "8 2150955.25\n",
      "9 1388170.25\n",
      "10 1038436.5625\n",
      "11 836493.4375\n",
      "12 696987.25\n",
      "13 590481.3125\n",
      "14 505298.75\n",
      "15 435625.1875\n",
      "16 377842.03125\n",
      "17 329468.28125\n",
      "18 288575.84375\n",
      "19 253798.46875\n",
      "20 224034.59375\n",
      "21 198461.765625\n",
      "22 176409.4375\n",
      "23 157289.453125\n",
      "24 140648.46875\n",
      "25 126103.375\n",
      "26 113348.140625\n",
      "27 102120.171875\n",
      "28 92200.765625\n",
      "29 83411.5703125\n",
      "30 75604.4765625\n",
      "31 68645.703125\n",
      "32 62428.98046875\n",
      "33 56864.5625\n",
      "34 51884.5078125\n",
      "35 47415.15625\n",
      "36 43385.421875\n",
      "37 39746.76953125\n",
      "38 36458.9453125\n",
      "39 33481.1875\n",
      "40 30778.48828125\n",
      "41 28322.673828125\n",
      "42 26088.13671875\n",
      "43 24052.21484375\n",
      "44 22194.03125\n",
      "45 20495.939453125\n",
      "46 18943.943359375\n",
      "47 17523.84375\n",
      "48 16222.1083984375\n",
      "49 15027.3486328125\n",
      "50 13929.9267578125\n",
      "51 12921.1875\n",
      "52 11995.453125\n",
      "53 11143.359375\n",
      "54 10357.5263671875\n",
      "55 9632.2626953125\n",
      "56 8962.767578125\n",
      "57 8344.15625\n",
      "58 7771.82666015625\n",
      "59 7241.7421875\n",
      "60 6751.2353515625\n",
      "61 6296.91650390625\n",
      "62 5875.80224609375\n",
      "63 5485.34375\n",
      "64 5122.85400390625\n",
      "65 4786.0390625\n",
      "66 4473.1123046875\n",
      "67 4182.07177734375\n",
      "68 3911.15087890625\n",
      "69 3659.056640625\n",
      "70 3424.565673828125\n",
      "71 3206.090576171875\n",
      "72 3002.4248046875\n",
      "73 2812.59619140625\n",
      "74 2635.486083984375\n",
      "75 2470.2333984375\n",
      "76 2316.117431640625\n",
      "77 2172.276611328125\n",
      "78 2037.7816162109375\n",
      "79 1912.05126953125\n",
      "80 1794.4957275390625\n",
      "81 1684.537841796875\n",
      "82 1581.6917724609375\n",
      "83 1485.4852294921875\n",
      "84 1395.4356689453125\n",
      "85 1311.165771484375\n",
      "86 1232.2186279296875\n",
      "87 1158.24609375\n",
      "88 1088.9288330078125\n",
      "89 1023.9723510742188\n",
      "90 963.0514526367188\n",
      "91 905.9095458984375\n",
      "92 852.2933349609375\n",
      "93 802.0039672851562\n",
      "94 754.8161010742188\n",
      "95 710.5211181640625\n",
      "96 668.9339599609375\n",
      "97 629.862060546875\n",
      "98 593.1591796875\n",
      "99 558.6791381835938\n",
      "100 526.28271484375\n",
      "101 495.83282470703125\n",
      "102 467.2244567871094\n",
      "103 440.3065490722656\n",
      "104 414.9942321777344\n",
      "105 391.180419921875\n",
      "106 368.78759765625\n",
      "107 347.72052001953125\n",
      "108 327.89923095703125\n",
      "109 309.2438049316406\n",
      "110 291.6860046386719\n",
      "111 275.1549072265625\n",
      "112 259.59014892578125\n",
      "113 244.93502807617188\n",
      "114 231.13014221191406\n",
      "115 218.1250762939453\n",
      "116 205.87744140625\n",
      "117 194.33966064453125\n",
      "118 183.47117614746094\n",
      "119 173.2241668701172\n",
      "120 163.56854248046875\n",
      "121 154.46310424804688\n",
      "122 145.87954711914062\n",
      "123 137.7860565185547\n",
      "124 130.15359497070312\n",
      "125 122.9572982788086\n",
      "126 116.17231750488281\n",
      "127 109.77423858642578\n",
      "128 103.7352523803711\n",
      "129 98.03629302978516\n",
      "130 92.65739440917969\n",
      "131 87.58181762695312\n",
      "132 82.79454803466797\n",
      "133 78.27461242675781\n",
      "134 74.0073013305664\n",
      "135 69.97945404052734\n",
      "136 66.17546081542969\n",
      "137 62.58456802368164\n",
      "138 59.192787170410156\n",
      "139 55.98908233642578\n",
      "140 52.96427917480469\n",
      "141 50.10704040527344\n",
      "142 47.407798767089844\n",
      "143 44.857418060302734\n",
      "144 42.447566986083984\n",
      "145 40.169921875\n",
      "146 38.018280029296875\n",
      "147 35.98411560058594\n",
      "148 34.061893463134766\n",
      "149 32.245052337646484\n",
      "150 30.527700424194336\n",
      "151 28.903379440307617\n",
      "152 27.36785125732422\n",
      "153 25.915409088134766\n",
      "154 24.542072296142578\n",
      "155 23.242876052856445\n",
      "156 22.013805389404297\n",
      "157 20.852157592773438\n",
      "158 19.7530517578125\n",
      "159 18.713420867919922\n",
      "160 17.729137420654297\n",
      "161 16.797712326049805\n",
      "162 15.916436195373535\n",
      "163 15.082803726196289\n",
      "164 14.293609619140625\n",
      "165 13.546628952026367\n",
      "166 12.839631080627441\n",
      "167 12.170364379882812\n",
      "168 11.537128448486328\n",
      "169 10.936736106872559\n",
      "170 10.368645668029785\n",
      "171 9.830864906311035\n",
      "172 9.321370124816895\n",
      "173 8.839080810546875\n",
      "174 8.382015228271484\n",
      "175 7.948996543884277\n",
      "176 7.539007663726807\n",
      "177 7.150705814361572\n",
      "178 6.782642841339111\n",
      "179 6.4340949058532715\n",
      "180 6.103932857513428\n",
      "181 5.790885925292969\n",
      "182 5.494081020355225\n",
      "183 5.213124752044678\n",
      "184 4.946547985076904\n",
      "185 4.693833351135254\n",
      "186 4.454604148864746\n",
      "187 4.227701187133789\n",
      "188 4.012484550476074\n",
      "189 3.8085925579071045\n",
      "190 3.6150572299957275\n",
      "191 3.4315478801727295\n",
      "192 3.2576558589935303\n",
      "193 3.0926473140716553\n",
      "194 2.9364407062530518\n",
      "195 2.7880780696868896\n",
      "196 2.6473824977874756\n",
      "197 2.5139386653900146\n",
      "198 2.3872506618499756\n",
      "199 2.2671620845794678\n",
      "200 2.1532492637634277\n",
      "201 2.045116901397705\n",
      "202 1.9425199031829834\n",
      "203 1.8451616764068604\n",
      "204 1.7527509927749634\n",
      "205 1.6651208400726318\n",
      "206 1.5819380283355713\n",
      "207 1.5028804540634155\n",
      "208 1.4279580116271973\n",
      "209 1.3569318056106567\n",
      "210 1.2893069982528687\n",
      "211 1.225252389907837\n",
      "212 1.1643575429916382\n",
      "213 1.1065326929092407\n",
      "214 1.0516810417175293\n",
      "215 0.9995954036712646\n",
      "216 0.9500555992126465\n",
      "217 0.9030527472496033\n",
      "218 0.8584919571876526\n",
      "219 0.8160945773124695\n",
      "220 0.7759105563163757\n",
      "221 0.7375963926315308\n",
      "222 0.7013159990310669\n",
      "223 0.6667845845222473\n",
      "224 0.6339779496192932\n",
      "225 0.6028658151626587\n",
      "226 0.5732435584068298\n",
      "227 0.5451626777648926\n",
      "228 0.5184922814369202\n",
      "229 0.4931512773036957\n",
      "230 0.46902012825012207\n",
      "231 0.4460955560207367\n",
      "232 0.42426633834838867\n",
      "233 0.4035892188549042\n",
      "234 0.38390153646469116\n",
      "235 0.36522120237350464\n",
      "236 0.347446084022522\n",
      "237 0.3305562734603882\n",
      "238 0.31449654698371887\n",
      "239 0.29925405979156494\n",
      "240 0.28468555212020874\n",
      "241 0.2708798944950104\n",
      "242 0.2577461004257202\n",
      "243 0.24530233442783356\n",
      "244 0.2334405481815338\n",
      "245 0.22215354442596436\n",
      "246 0.21149267256259918\n",
      "247 0.20124173164367676\n",
      "248 0.19150611758232117\n",
      "249 0.18229901790618896\n",
      "250 0.17350639402866364\n",
      "251 0.16516242921352386\n",
      "252 0.1572355180978775\n",
      "253 0.14966784417629242\n",
      "254 0.14247378706932068\n",
      "255 0.13563494384288788\n",
      "256 0.12915335595607758\n",
      "257 0.12296656519174576\n",
      "258 0.11706440895795822\n",
      "259 0.11146437376737595\n",
      "260 0.1061476469039917\n",
      "261 0.1010623648762703\n",
      "262 0.0962378978729248\n",
      "263 0.09164150804281235\n",
      "264 0.0872705802321434\n",
      "265 0.0831080824136734\n",
      "266 0.07915430516004562\n",
      "267 0.07538415491580963\n",
      "268 0.07180117070674896\n",
      "269 0.06840094923973083\n",
      "270 0.0651235431432724\n",
      "271 0.062054067850112915\n",
      "272 0.059107597917318344\n",
      "273 0.056309428066015244\n",
      "274 0.05363747105002403\n",
      "275 0.05108826234936714\n",
      "276 0.04868108034133911\n",
      "277 0.04638535901904106\n",
      "278 0.04421284794807434\n",
      "279 0.04210805892944336\n",
      "280 0.04014509543776512\n",
      "281 0.03823612257838249\n",
      "282 0.03645306080579758\n",
      "283 0.03473846986889839\n",
      "284 0.033104319125413895\n",
      "285 0.03154899924993515\n",
      "286 0.030062342062592506\n",
      "287 0.02866574004292488\n",
      "288 0.027335090562701225\n",
      "289 0.026034435257315636\n",
      "290 0.02481953054666519\n",
      "291 0.023662583902478218\n",
      "292 0.022561445832252502\n",
      "293 0.021505825221538544\n",
      "294 0.020519640296697617\n",
      "295 0.019555596634745598\n",
      "296 0.018653957173228264\n",
      "297 0.0177775826305151\n",
      "298 0.016957923769950867\n",
      "299 0.016180243343114853\n",
      "300 0.015423349104821682\n",
      "301 0.01471330039203167\n",
      "302 0.014040132984519005\n",
      "303 0.013392726890742779\n",
      "304 0.012782149016857147\n",
      "305 0.01218734122812748\n",
      "306 0.01163332536816597\n",
      "307 0.01110411249101162\n",
      "308 0.010602451860904694\n",
      "309 0.01012560073286295\n",
      "310 0.00965960044413805\n",
      "311 0.009232056327164173\n",
      "312 0.008811512030661106\n",
      "313 0.008410361595451832\n",
      "314 0.008038458414375782\n",
      "315 0.007669189479202032\n",
      "316 0.007332965731620789\n",
      "317 0.006998940370976925\n",
      "318 0.006696391850709915\n",
      "319 0.006400533486157656\n",
      "320 0.006112109869718552\n",
      "321 0.005844021216034889\n",
      "322 0.005590473767369986\n",
      "323 0.005343416705727577\n",
      "324 0.005112908780574799\n",
      "325 0.00488791149109602\n",
      "326 0.004676502197980881\n",
      "327 0.004478266928344965\n",
      "328 0.004284029360860586\n",
      "329 0.004097981844097376\n",
      "330 0.003921440802514553\n",
      "331 0.0037599399220198393\n",
      "332 0.00359380547888577\n",
      "333 0.003440788947045803\n",
      "334 0.003297006245702505\n",
      "335 0.003161362139508128\n",
      "336 0.003027908504009247\n",
      "337 0.002900963881984353\n",
      "338 0.0027744683902710676\n",
      "339 0.002664205152541399\n",
      "340 0.0025562667287886143\n",
      "341 0.002455316483974457\n",
      "342 0.0023524954449385405\n",
      "343 0.0022561370860785246\n",
      "344 0.0021659107878804207\n",
      "345 0.002079532016068697\n",
      "346 0.00199340982362628\n",
      "347 0.0019175632623955607\n",
      "348 0.001841660006903112\n",
      "349 0.0017705702921375632\n",
      "350 0.001703325193375349\n",
      "351 0.0016393120167776942\n",
      "352 0.0015778840752318501\n",
      "353 0.0015156514709815383\n",
      "354 0.001458874437958002\n",
      "355 0.0014045523712411523\n",
      "356 0.0013532148441299796\n",
      "357 0.0013020841870456934\n",
      "358 0.0012575718574225903\n",
      "359 0.0012097926810383797\n",
      "360 0.001168734859675169\n",
      "361 0.0011252355761826038\n",
      "362 0.0010848443489521742\n",
      "363 0.0010473547736182809\n",
      "364 0.0010107237612828612\n",
      "365 0.000975174130871892\n",
      "366 0.0009408951154910028\n",
      "367 0.0009092373657040298\n",
      "368 0.0008782963850535452\n",
      "369 0.0008485402213409543\n",
      "370 0.0008185495971702039\n",
      "371 0.0007915754686109722\n",
      "372 0.0007659581024199724\n",
      "373 0.0007400585454888642\n",
      "374 0.0007158772205002606\n",
      "375 0.000692735193297267\n",
      "376 0.0006696302443742752\n",
      "377 0.0006494135595858097\n",
      "378 0.0006280478555709124\n",
      "379 0.0006080996827222407\n",
      "380 0.000588618335314095\n",
      "381 0.0005709340912289917\n",
      "382 0.0005531900096684694\n",
      "383 0.0005364143289625645\n",
      "384 0.0005200716550461948\n",
      "385 0.0005043927812948823\n",
      "386 0.0004894005251117051\n",
      "387 0.00047509209252893925\n",
      "388 0.00046217229100875556\n",
      "389 0.00044724330655299127\n",
      "390 0.00043481989996507764\n",
      "391 0.0004226212331559509\n",
      "392 0.0004104715771973133\n",
      "393 0.0003986231458839029\n",
      "394 0.00038698993739672005\n",
      "395 0.0003770225739572197\n",
      "396 0.0003661100345198065\n",
      "397 0.0003556092851795256\n",
      "398 0.0003453712852206081\n",
      "399 0.0003370234335307032\n",
      "400 0.00032750004902482033\n",
      "401 0.0003191143332514912\n",
      "402 0.0003108055971097201\n",
      "403 0.00030212660203687847\n",
      "404 0.00029320974135771394\n",
      "405 0.0002863919362425804\n",
      "406 0.00027843876159749925\n",
      "407 0.00027046227478422225\n",
      "408 0.00026394377346150577\n",
      "409 0.00025777515838854015\n",
      "410 0.0002508650068193674\n",
      "411 0.00024490244686603546\n",
      "412 0.00023867083655204624\n",
      "413 0.00023290805984288454\n",
      "414 0.00022691527556162328\n",
      "415 0.00022232052288018167\n",
      "416 0.00021673686569556594\n",
      "417 0.00021171968546696007\n",
      "418 0.00020684504124801606\n",
      "419 0.00020191473595332354\n",
      "420 0.00019724739831872284\n",
      "421 0.00019287237955722958\n",
      "422 0.00018836163508240134\n",
      "423 0.00018419174011796713\n",
      "424 0.00018044425814878196\n",
      "425 0.00017665402265265584\n",
      "426 0.00017228514479938895\n",
      "427 0.0001690286153461784\n",
      "428 0.00016527179104741663\n",
      "429 0.00016159570077434182\n",
      "430 0.0001579313538968563\n",
      "431 0.0001542609534226358\n",
      "432 0.00015109636296983808\n",
      "433 0.00014805281534790993\n",
      "434 0.00014499432290904224\n",
      "435 0.00014203759201336652\n",
      "436 0.00013917917385697365\n",
      "437 0.00013654342910740525\n",
      "438 0.00013346911873668432\n",
      "439 0.00013086156104691327\n",
      "440 0.00012819170660804957\n",
      "441 0.00012560161121655256\n",
      "442 0.00012312484614085406\n",
      "443 0.00012075617996742949\n",
      "444 0.00011826651461888105\n",
      "445 0.00011592906957957894\n",
      "446 0.0001139665546361357\n",
      "447 0.0001121158929890953\n",
      "448 0.0001095060069928877\n",
      "449 0.00010764660692075267\n",
      "450 0.00010578989167697728\n",
      "451 0.00010352165554650128\n",
      "452 0.00010157099313801154\n",
      "453 9.96505405055359e-05\n",
      "454 9.763715206645429e-05\n",
      "455 9.615121234674007e-05\n",
      "456 9.44413841352798e-05\n",
      "457 9.28664012462832e-05\n",
      "458 9.135127766057849e-05\n",
      "459 8.972026262199506e-05\n",
      "460 8.79632425494492e-05\n",
      "461 8.689663809491321e-05\n",
      "462 8.49953357828781e-05\n",
      "463 8.35569080663845e-05\n",
      "464 8.197921852115542e-05\n",
      "465 8.067486487561837e-05\n",
      "466 7.926102261990309e-05\n",
      "467 7.779095903970301e-05\n",
      "468 7.659038965357468e-05\n",
      "469 7.570187881356105e-05\n",
      "470 7.424823706969619e-05\n",
      "471 7.324446778511629e-05\n",
      "472 7.178306259447709e-05\n",
      "473 7.082312367856503e-05\n",
      "474 6.957329605938867e-05\n",
      "475 6.880667206132784e-05\n",
      "476 6.75753690302372e-05\n",
      "477 6.660557846771553e-05\n",
      "478 6.580723129445687e-05\n",
      "479 6.430113717215136e-05\n",
      "480 6.358663085848093e-05\n",
      "481 6.270402809605002e-05\n",
      "482 6.172434223117307e-05\n",
      "483 6.0867667343700305e-05\n",
      "484 5.986301403027028e-05\n",
      "485 5.902071643504314e-05\n",
      "486 5.788826456409879e-05\n",
      "487 5.7295404985779896e-05\n",
      "488 5.660652095684782e-05\n",
      "489 5.5735432397341356e-05\n",
      "490 5.509959737537429e-05\n",
      "491 5.4270680266199633e-05\n",
      "492 5.3545543778454885e-05\n",
      "493 5.283985592541285e-05\n",
      "494 5.226235225563869e-05\n",
      "495 5.150648212293163e-05\n",
      "496 5.061714182374999e-05\n",
      "497 4.9998099711956456e-05\n",
      "498 4.9112801207229495e-05\n",
      "499 4.8660975153325126e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0581629276275635\n",
      "10000 0.4231279790401459\n",
      "20000 0.17353735864162445\n",
      "30000 0.0670170933008194\n",
      "40000 0.026668930426239967\n"
     ]
    }
   ],
   "source": [
    "# Code in file nn/two_layer_net_module.py\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Tensor of input data and we must return\n",
    "    a Tensor of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary (differentiable) operations on Tensors.\n",
    "    \"\"\"\n",
    "    h_relu = self.linear1(x).clamp(min=0)\n",
    "    y_pred = self.linear2(h_relu)\n",
    "    return y_pred\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(50000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 10000 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0037, -0.1132,  0.5343, -0.6003, -0.6639,  1.6061,  0.6295,\n",
       "         0.4072,  1.0311,  0.7904])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x[63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0302, -0.1166,  0.5118, -0.5859, -0.6820,  1.5857,  0.5887,\n",
       "         0.3936,  0.9835,  0.7885])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[63]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very good. Let's go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in file nn/two_layer_net_module.py\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    In the forward function we accept a Tensor of input data and we must return\n",
    "    a Tensor of output data. We can use Modules defined in the constructor as\n",
    "    well as arbitrary (differentiable) operations on Tensors.\n",
    "    \"\"\"\n",
    "    h_relu = self.linear1(x).clamp(min=0)\n",
    "    y_pred = self.linear2(h_relu)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.from_numpy([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9693735241889954\n",
      "10000 0.6084838509559631\n",
      "20000 0.4031957983970642\n",
      "30000 0.27504071593284607\n",
      "40000 0.19393056631088257\n",
      "50000 0.1413998305797577\n",
      "60000 0.10629907995462418\n",
      "70000 0.082027368247509\n",
      "80000 0.06474994868040085\n",
      "90000 0.05214588716626167\n",
      "99999 0.042807869613170624\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 10, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "# x = torch.randn(N, D_in)\n",
    "# y = torch.randn(N, D_out)\n",
    "\n",
    "x = torch.from_numpy([])\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for t in range(100000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 10000 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(t, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2524, -0.6917,  0.9070, -0.3784,  0.3710, -0.2966, -0.3182,\n",
       "        -0.6720,  0.8225,  0.5203])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x[63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4218, -2.0758, -0.3345, -0.3585, -0.5878,  0.8537,  0.8468,\n",
       "         0.5459,  1.0140, -0.8848])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[63]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok cool\n",
    "\n",
    "Now let's create a seconds to categorical periods transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 % 4\n",
      "0 % 16\n",
      "1 % 64\n",
      "0 % 256\n",
      "2 % 1024\n",
      "3 % 4096\n",
      "0 % 16384\n",
      "3 % 65536\n",
      "3 % 262144\n"
     ]
    }
   ],
   "source": [
    "a = round(datetime.datetime.now().timestamp())\n",
    "prev = a\n",
    "for i in range(1,10):\n",
    "#     print(prev,  (prev % ((4**i))), '%', i, (2**i))\n",
    "\n",
    "    print ( math.floor(prev % (4**i) / (4**i) * 4) , '%', (4**i))\n",
    "\n",
    "#     prev -= prev % (4**i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
